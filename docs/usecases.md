Use Cases
=========

Following are a few use cases that Polygenea makes straightforward
but are more difficult under other common data models,
as well as some that are not first-class in Polygenea but are nonetheless easily implemented.


## Fixing a Mistake

If a node is in error, you simply

1.	Add another node that is not in error

2.	Add a Connection node noting the new node is an "update-of" the old one

The error gets corrected and the fact that the error existed and was corrected gets recorded
without any chance of stepping on the toes of another researcher.


## Distributed Research

Because nodes are immutable, there is no danger of incompatible edits.
If a hundred users and databases each have a copy of some nodes
and each add new nodes to the mix,
all that is needed to re-synchronize the various copies
is to send one another the new nodes.

The only possible challenge is in ensuring that the id fields of independently-created Thing nodes
are unique.
I consider this a minor challenge at best 
because many solutions to the unique identifier problem are known,
including UUID versions 1 and 4 among others.


## Privacy and Data Ownership

Private data is easily handled: it's just a set of nodes that do not get shared with others.

Because data is immutable, sharing it with others does not risk it being changed in any way.
Thus, most of the importance of data ownership is moot.
If you wish to keep a copy of some data, doing so does not hurt anyone else.

Paywall data is a bit more difficult.
It can easily be tagged with a Property with a key of `meta-paywall-owner`
so that tools will know it was paywalled, but keeping someone 
from spreading it once they have it is not feasible.

I usually assume that each user will have a "belief set", 
a set of nodes that the user in question has decided to accept.
This set could be kept secret or revealed to others, 
but I would be hesitant to make a single set shared by multiple users.
However, the belief set idea is really a matter of the user experience;
it is not part of the data itself.


## Handling Disagreement

If you and I disagree about some fact,
all we need to do is have a different set of nodes we chose to believe.
We can still share with one another new nodes we create
and most of these will probably apply to both of our views of the world
whether the disagreement is rooted in a match, a property, a connection, or a source.
Edit wars are difficult to create in a world without edits.


## Handling Uncertainty

Even more useful than being able to disagree with collaborators
is being able to disagree with one's self.
I can, for example, have data asserting that persons A and B are the same individual,
that persons B and C are the same individual, and that A and C are distinct individuals.
Even though this state contains a logical contradiction and cannot be how history actually looked,
I can leave it in the data to represent my confusion
until such time as I have sufficient evidence to resolve the puzzle.
In general, I can enter both sides of a difficult choice into the data
and explore what the world would look like under each scenario,
only later deciding that one decision is superior in some way to the others.

Polygenea can also model probabilistic uncertainty with `meta-probability` properties,
though the merits of such properties are not at present evident to me.


## Recording Provenance

Provenance of sources is easily modelled
as two sources with a "derived-from" Connection between them.
Those connection nodes themselves may be sourced 
to express the rationale behind the provenance decisions,
or may be left without a source.


## Attribution

Attribution (to the creator of a node) is easily handled with `meta-creator` properties.
Nodes that are independently created several times by several researchers
will simply accrue a set of creator properties.


## Safe, Small, Interesting Tasks

The polygenea data model was designed with the goal of having each atomic research step
result in the creation of a few new nodes.
If I succeeded in this goal then *any* small-but-useful contribution one might make
can be represented by the addition of a few nodes.
Additionally, the impact of a mistake is limited since others can simply ignore it.


## Rule Creation

I envision a simple way to guide novices through the process of creating a Rule node:

1.	User creates a ruleless Inference and the nodes that will cite it as their source.

2.	A overly-constrained Rule is generated by

	1.	Copying each antecedent node of the Inference into the Rule's antecedent
	2.	Copying each nodes sourcing the Inference to the Rule's consequent pool
	3.	Adding any nodes that the consequent references that are not yet either an antecedent or consequent to the antecedent pool
	4.	Removing all fields of antecedent nodes that reference nodes not in the antecedent pool
	5.	Replacing all remaining references with local references

3.	The Rule is generalized by asking the user questions like the following:
	
	*	"Is the fact that the name was 'John' important to this inference?"
	*	"Both things in this example had the same date property; is that important to this inference?"
		
	â€¦and so on to determine parts of the antecedents that can be loosened up.
	




